{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Here SLIM_EN was trained, but the backbone of the training can be applied with whatever model\n"
      ],
      "metadata": {
        "id": "iRtdYeyvdM6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#questo\n",
        "%%cython\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "from libc.stdint cimport int32_t\n",
        "import time\n",
        "\n",
        "cdef inline double sign(double x):\n",
        "    if x > 0:\n",
        "        return 1.0\n",
        "    elif x < 0:\n",
        "        return -1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def do_some_training_EN(\n",
        "    URM_train,\n",
        "    double initial_learning_rate,\n",
        "    double regularization_2,\n",
        "    double l1_ratio,\n",
        "    double decay_rate,\n",
        "    int num_iterations,\n",
        "    double[:, :] existing_item_item_S,\n",
        "    double[:, :] S_icm,     # New parameter\n",
        "    double alpha           # New parameter\n",
        "):\n",
        "    cdef int n_items = URM_train.shape[1]\n",
        "    URM_train_csr = URM_train.tocsr()\n",
        "    URM_train_coo = URM_train.tocoo()\n",
        "    cdef long start_time = time.time()\n",
        "    cdef int32_t[:] indices = URM_train_csr.indices.view(dtype=np.int32)\n",
        "    cdef int32_t[:] indptr = URM_train_csr.indptr.view(dtype=np.int32)\n",
        "    cdef double[:] data = URM_train_csr.data.view(dtype=np.float64)\n",
        "    cdef int32_t[:] coo_row = URM_train_coo.row.view(dtype=np.int32)\n",
        "    cdef int32_t[:] coo_col = URM_train_coo.col.view(dtype=np.int32)\n",
        "    cdef double[:] coo_data = URM_train_coo.data.view(dtype=np.float64)\n",
        "\n",
        "    cdef double[:, :] item_item_S\n",
        "    if existing_item_item_S is not None:\n",
        "        item_item_S = existing_item_item_S\n",
        "    else:\n",
        "        item_item_S = np.zeros((n_items, n_items), dtype=np.float64)\n",
        "\n",
        "    cdef double learning_rate = initial_learning_rate\n",
        "    cdef double loss = 0.0\n",
        "    cdef double prediction_error, predicted_rating, profile_rating\n",
        "    cdef int user_id, item_id, profile_item_id, sample_index, index\n",
        "    cdef int start_idx, end_idx\n",
        "    cdef int32_t[:] random_indices = np.random.randint(0, URM_train_coo.nnz, size=num_iterations).astype(np.int32)\n",
        "     # Early stopping variables\n",
        "    cdef int patience_counter = 0\n",
        "    cdef double last_loss = np.inf\n",
        "    cdef int patience = 20\n",
        "    cdef double min_delta = 1e-5\n",
        "    cdef int warm_restart = 0\n",
        "    for sample_num in range(num_iterations+1):\n",
        "        sample_index = random_indices[sample_num]\n",
        "        user_id = coo_row[sample_index]\n",
        "        item_id = coo_col[sample_index]\n",
        "        true_rating = coo_data[sample_index]\n",
        "\n",
        "        predicted_rating = 0.0\n",
        "        start_idx = indptr[user_id]\n",
        "        end_idx = indptr[user_id + 1]\n",
        "\n",
        "        for index in range(start_idx, end_idx):\n",
        "            profile_item_id = indices[index]\n",
        "            profile_rating = data[index]\n",
        "            predicted_rating += profile_rating * item_item_S[profile_item_id, item_id]\n",
        "\n",
        "        prediction_error = true_rating - predicted_rating\n",
        "        loss += prediction_error ** 2\n",
        "\n",
        "        for index in range(start_idx, end_idx):\n",
        "            profile_item_id = indices[index]\n",
        "            profile_rating = data[index]\n",
        "            item_item_S[profile_item_id, item_id] += learning_rate * (\n",
        "                prediction_error * profile_rating\n",
        "                - (1 - l1_ratio) * regularization_2 * item_item_S[profile_item_id, item_id]\n",
        "                - l1_ratio * sign(item_item_S[profile_item_id, item_id])\n",
        "                - alpha * (item_item_S[profile_item_id, item_id] - S_icm[profile_item_id, item_id]) #new for icm\n",
        "            )\n",
        "        if sample_num % 5000 == 0:\n",
        "            learning_rate *= decay_rate\n",
        "            if sample_num > 0:\n",
        "              current_loss = loss / sample_num\n",
        "            else:\n",
        "              current_loss=0\n",
        "            #loss_history.append(current_loss)\n",
        "            if sample_num % 1000000 == 0:\n",
        "                  elapsed_time = time.time() - start_time\n",
        "                  samples_per_second = sample_num / elapsed_time\n",
        "                  print(\n",
        "                      \"Iteration {} in {:.2f} seconds, loss is {:.4f}. Samples per second {:.2f}\".format(\n",
        "                          sample_num, elapsed_time, current_loss, samples_per_second\n",
        "                      )\n",
        "                  )\n",
        "            # Early stopping check\n",
        "            if abs(last_loss - current_loss) < min_delta or current_loss > 1:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    if warm_restart == 0:\n",
        "                        learning_rate = initial_learning_rate\n",
        "                        warm_restart += 1\n",
        "                        patience_counter = 0\n",
        "                        print(\"warm restart\")\n",
        "                    else:\n",
        "                        print(\"Early stopping at iteration {}. Loss has not improved significantly for {} iterations, or has stayed above 1 too much. loss was {}\".format(sample_num, patience * 5000, current_loss))\n",
        "                        break\n",
        "            else:\n",
        "                patience_counter = 0  # Reset patience counter if loss improves\n",
        "            last_loss = current_loss\n",
        "\n",
        "    return loss, item_item_S\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sljJSJjJ3mXx",
        "outputId": "c1b81e4e-53f1-4233-d6ea-369daf2926b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "performance hint: /root/.cache/ipython/cython/_cython_magic_793d7496f50a3060672d684fead44deb3ce1ba1a.pyx:56:38: Index should be typed for more efficient access\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of stderr:\n",
            "In file included from /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929,\n",
            "                 from /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
            "                 from /usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h:5,\n",
            "                 from /root/.cache/ipython/cython/_cython_magic_793d7496f50a3060672d684fead44deb3ce1ba1a.c:1250:\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
            "   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  ^~~~~~~"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2_XefUqv0Is"
      },
      "outputs": [],
      "source": [
        "from Recommenders.BaseRecommender import BaseRecommender\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, csc_matrix\n",
        "\n",
        "\n",
        "class MyItemItemRecommender(BaseRecommender):\n",
        "    RECOMMENDER_NAME = \"MyItemItemRecommender\"\n",
        "\n",
        "    def __init__(self, URM_train, item_item_S):\n",
        "        super(MyItemItemRecommender, self).__init__(URM_train)\n",
        "        self.item_item_S = item_item_S\n",
        "\n",
        "    def fit(self):\n",
        "        # No training is needed since we already have item_item_S\n",
        "        pass\n",
        "\n",
        "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
        "        # Compute the scores for each user in user_id_array\n",
        "        user_profiles = self.URM_train[user_id_array]\n",
        "        scores = user_profiles.dot(self.item_item_S)\n",
        "\n",
        "        # If items_to_compute is specified, filter the scores\n",
        "        if items_to_compute is not None:\n",
        "            # Initialize scores_all with -inf\n",
        "            scores_all = np.full((len(user_id_array), self.URM_train.shape[1]), -np.inf, dtype=np.float32)\n",
        "            for idx, user_id in enumerate(user_id_array):\n",
        "                scores_all[idx, items_to_compute[user_id]] = scores[idx, items_to_compute[user_id]]\n",
        "            scores = scores_all\n",
        "\n",
        "        return scores\n",
        "    def _remove_seen_on_scores(self, user_id_array, scores):\n",
        "        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n",
        "\n",
        "        # Iterate over each user in the user_id_array\n",
        "        for idx, user_id in enumerate(user_id_array):\n",
        "            start_pos = self.URM_train.indptr[user_id]\n",
        "            end_pos = self.URM_train.indptr[user_id + 1]\n",
        "            seen_items = self.URM_train.indices[start_pos:end_pos]\n",
        "            scores[idx, seen_items] = -np.inf\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def recommend(\n",
        "    self,\n",
        "    user_id_array,\n",
        "    cutoff=None,\n",
        "    remove_seen_flag=True,\n",
        "    remove_top_pop_flag=False,\n",
        "    remove_custom_items_flag=False,\n",
        "    return_scores=False,\n",
        "    ):\n",
        "        # Ensure user_id_array is an array\n",
        "        if np.isscalar(user_id_array):\n",
        "            user_id_array = np.array([user_id_array])\n",
        "\n",
        "        # Compute scores for the users\n",
        "        scores = self._compute_item_score(user_id_array)\n",
        "\n",
        "        # Convert scores to dense array if necessary\n",
        "        if isinstance(scores, csr_matrix) or isinstance(scores, csc_matrix):\n",
        "            scores = scores.toarray()\n",
        "\n",
        "        # Exclude seen items\n",
        "        if remove_seen_flag:\n",
        "            scores = self._remove_seen_on_scores(user_id_array, scores)\n",
        "\n",
        "        # Apply cutoff\n",
        "        if cutoff is None:\n",
        "            cutoff = scores.shape[1]\n",
        "\n",
        "        # Get the top items\n",
        "        ranking = np.zeros((scores.shape[0], cutoff), dtype=np.int32)\n",
        "        for idx in range(scores.shape[0]):\n",
        "            user_scores = scores[idx]\n",
        "            top_items = np.argsort(-user_scores)[:cutoff]\n",
        "            ranking[idx] = top_items\n",
        "\n",
        "        if return_scores:\n",
        "            return ranking, scores\n",
        "        else:\n",
        "            return ranking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from Evaluation.Evaluator import EvaluatorHoldout\n",
        "import numpy as np\n",
        "\n",
        "#500 000 9 secondi\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.002, log=True)\n",
        "    regularization = trial.suggest_float(\"regularization\", 1e-6, 1e-3, log=True)\n",
        "    decay_rate = trial.suggest_float(\"decay_rate\", 0.999999, 1.0)\n",
        "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 0.5)\n",
        "\n",
        "    # K-Fold Cross Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"MAP@10\": []}\n",
        "    print(f\"trying learning rate={learning_rate}, reg = {regularization}, l1_ratio={l1_ratio} and decay = {decay_rate} for this crossval round\")\n",
        "    for train_index, val_index in kf.split(URM_all):\n",
        "        # Create train and validation splits\n",
        "        URM_train_split = URM_all_csr[train_index]\n",
        "        URM_validation_split = URM_all_csr[val_index]\n",
        "\n",
        "        # Train model on the split\n",
        "        loss, item_item_S= do_some_training_EN(\n",
        "            URM_train_split, learning_rate, regularization, l1_ratio, decay_rate, 5000000, None\n",
        "        )\n",
        "\n",
        "        # Initialize and fit the recommender\n",
        "        recommender = MyItemItemRecommender(URM_train_split, item_item_S)\n",
        "        recommender.fit()\n",
        "\n",
        "        # Evaluate on the validation split\n",
        "        evaluator = EvaluatorHoldout(URM_validation_split, cutoff_list=[10])\n",
        "        results, _ = evaluator.evaluateRecommender(recommender)\n",
        "\n",
        "        # Collect MAP@10 metric\n",
        "        fold_metrics[\"MAP@10\"].append(results.loc[10, 'MAP'])\n",
        "        print(results.loc[10, 'MAP'])\n",
        "\n",
        "    # Return the mean MAP@10 across folds as the objective value\n",
        "    mean_map_at_10 = np.mean(fold_metrics[\"MAP@10\"])\n",
        "    trial.set_user_attr(\"MAP@10\", mean_map_at_10)  # Store MAP@10 in the trial\n",
        "\n",
        "    return mean_map_at_10\n",
        "\n",
        "\n",
        "# Initialize Optuna study\n",
        "study = optuna.create_study(direction=\"maximize\")  # We aim to maximize MAP@10\n",
        "\n",
        "# Run optimization for 100 trials\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", study.best_params)\n",
        "\n",
        "# Best score\n",
        "print(\"Best MAP@10:\", study.best_value)\n",
        "\n",
        "# Save study for later analysis\n",
        "df = study.trials_dataframe()\n",
        "df[\"MAP@10\"] = [trial.user_attrs.get(\"MAP@10\", None) for trial in study.trials]\n",
        "df.to_csv(\"optuna_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "AD9RoBVa1a0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Train SLIM-EN, having found the best parameters through optuna,  with validation and early stopping based on MAP@10.\n",
        "\"\"\"\n",
        "# Initialize similarity matrix\n",
        "last_map = -np.inf  # Track the last MAP@10 score\n",
        "patience_counter = 0  # Counter for early stopping\n",
        "map_history = []  # To store MAP@10 scores over epochs\n",
        "patience = 5  # Number of epochs to wait for improvement\n",
        "max_epochs = 100  # Maximum number of epochs\n",
        "best_item_item_S = None  # Store the best similarity matrix\n",
        "current_item_item_S = None  # Store the  similarity matrix\n",
        "URM_train_split, URM_validation_split = split_train_in_two_percentage_global_sample(URM_all, train_percentage=0.8)\n",
        "num_iterations_per_epoch = 5000000  # Number of iterations per epoch\n",
        "\n",
        "# Ensure train and validation splits are properly set\n",
        "evaluator = EvaluatorHoldout(URM_validation_split, cutoff_list=[10])  # Use validation split evaluator\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(f\"Starting epoch {epoch + 1}\")\n",
        "    S_icm = cosine_similarity(ICM)\n",
        "    # Train for one epoch\n",
        "    #loss, current_item_item_S = do_some_training_EN(\n",
        "    #    URM_train_split, 0.00010012085679135815, 2.668605667480152e-06, 0.24613633537942015, 0.9999991522253808, num_iterations_per_epoch, item_item_S\n",
        "    #)\n",
        "    loss, current_item_item_S = do_some_training_EN(\n",
        "        URM_train_split, 0.00010202373981397776, 1.8374449620346475e-05, 0.034021854645934185, 0.9999996279382791, num_iterations_per_epoch, current_item_item_S, S_icm, 0.01808726112039536\n",
        "    )\n",
        "    # Initialize and fit the recommender with the updated similarity matrix\n",
        "    recommender = MyItemItemRecommender(URM_train_split, current_item_item_S)\n",
        "    recommender.fit()\n",
        "\n",
        "    # Evaluate on the validation split\n",
        "    results, _ = evaluator.evaluateRecommender(recommender)\n",
        "\n",
        "    # Collect MAP@10 metric\n",
        "    current_map = results.loc[10, \"MAP\"]\n",
        "    map_history.append(current_map)\n",
        "    print(f\"Epoch {epoch + 1}: MAP@10 = {current_map:.4f}\")\n",
        "\n",
        "    # Check for convergence (plateauing or overfitting)\n",
        "    if current_map > last_map:\n",
        "        last_map = current_map\n",
        "        patience_counter = 0  # Reset patience if MAP improves\n",
        "        best_item_item_S = current_item_item_S  # Save the best similarity matrix\n",
        "        print(\"Improvement detected. Saving the current similarity matrix.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement in MAP@10. Patience counter: {patience_counter}/{patience}\")\n",
        "\n",
        "    # Stop if patience is exhausted\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping: No improvement in MAP@10 for consecutive epochs.\")\n",
        "        break\n",
        "\n",
        "print(f\"Training complete. Best MAP@10: {max(map_history):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8pwofkxR3r",
        "outputId": "dd283c3a-4437-4e53-bb5b-93613dd6f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 145 (0.41 %) of 35736 users have no sampled items\n",
            "EvaluatorHoldout: Ignoring 145 ( 0.4%) Users that have less than 1 test interactions\n",
            "Starting epoch 1\n",
            "Iteration 0 in 0.85 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 25.76 seconds, loss is 0.9757. Samples per second 38814.45\n",
            "Iteration 2000000 in 39.71 seconds, loss is 0.9538. Samples per second 50367.25\n",
            "Iteration 3000000 in 53.31 seconds, loss is 0.9337. Samples per second 56271.70\n",
            "Iteration 4000000 in 66.91 seconds, loss is 0.9152. Samples per second 59781.16\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.83 min. Users per second: 325\n",
            "Epoch 1: MAP@10 = 0.0258\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 2\n",
            "Iteration 0 in 0.91 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.49 seconds, loss is 0.8009. Samples per second 68993.36\n",
            "Iteration 2000000 in 28.75 seconds, loss is 0.7881. Samples per second 69558.73\n",
            "Iteration 3000000 in 42.33 seconds, loss is 0.7758. Samples per second 70878.82\n",
            "Iteration 4000000 in 55.89 seconds, loss is 0.7642. Samples per second 71564.09\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.83 min. Users per second: 324\n",
            "Epoch 2: MAP@10 = 0.0307\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 3\n",
            "Iteration 0 in 0.26 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 13.75 seconds, loss is 0.6900. Samples per second 72700.99\n",
            "Iteration 2000000 in 27.38 seconds, loss is 0.6810. Samples per second 73054.83\n",
            "Iteration 3000000 in 41.80 seconds, loss is 0.6722. Samples per second 71765.03\n",
            "Iteration 4000000 in 55.42 seconds, loss is 0.6637. Samples per second 72172.43\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.84 min. Users per second: 322\n",
            "Epoch 3: MAP@10 = 0.0342\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 4\n",
            "Iteration 0 in 0.35 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.07 seconds, loss is 0.6085. Samples per second 71092.53\n",
            "Iteration 2000000 in 27.87 seconds, loss is 0.6016. Samples per second 71760.32\n",
            "Iteration 3000000 in 42.56 seconds, loss is 0.5948. Samples per second 70494.58\n",
            "Iteration 4000000 in 56.08 seconds, loss is 0.5882. Samples per second 71331.70\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.85 min. Users per second: 321\n",
            "Epoch 4: MAP@10 = 0.0366\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 5\n",
            "Iteration 0 in 0.26 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 13.98 seconds, loss is 0.5451. Samples per second 71531.31\n",
            "Iteration 2000000 in 27.71 seconds, loss is 0.5393. Samples per second 72183.37\n",
            "Iteration 3000000 in 41.41 seconds, loss is 0.5337. Samples per second 72449.60\n",
            "Iteration 4000000 in 55.18 seconds, loss is 0.5284. Samples per second 72490.95\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.83 min. Users per second: 324\n",
            "Epoch 5: MAP@10 = 0.0381\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 6\n",
            "Iteration 0 in 0.75 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.57 seconds, loss is 0.4929. Samples per second 68647.32\n",
            "Iteration 2000000 in 28.42 seconds, loss is 0.4884. Samples per second 70382.69\n",
            "Iteration 3000000 in 42.16 seconds, loss is 0.4839. Samples per second 71165.35\n",
            "Iteration 4000000 in 56.10 seconds, loss is 0.4795. Samples per second 71298.56\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.86 min. Users per second: 320\n",
            "Epoch 6: MAP@10 = 0.0393\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 7\n",
            "Iteration 0 in 0.10 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.03 seconds, loss is 0.4500. Samples per second 71260.52\n",
            "Iteration 2000000 in 27.89 seconds, loss is 0.4464. Samples per second 71708.04\n",
            "Iteration 3000000 in 42.42 seconds, loss is 0.4425. Samples per second 70722.66\n",
            "Iteration 4000000 in 56.35 seconds, loss is 0.4387. Samples per second 70986.27\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.86 min. Users per second: 320\n",
            "Epoch 7: MAP@10 = 0.0401\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 8\n",
            "Iteration 0 in 1.04 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.99 seconds, loss is 0.4142. Samples per second 66717.11\n",
            "Iteration 2000000 in 28.91 seconds, loss is 0.4108. Samples per second 69187.15\n",
            "Iteration 3000000 in 42.66 seconds, loss is 0.4077. Samples per second 70325.62\n",
            "Iteration 4000000 in 56.50 seconds, loss is 0.4044. Samples per second 70790.26\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.87 min. Users per second: 317\n",
            "Epoch 8: MAP@10 = 0.0407\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 9\n",
            "Iteration 0 in 0.50 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.53 seconds, loss is 0.3825. Samples per second 68809.64\n",
            "Iteration 2000000 in 29.05 seconds, loss is 0.3795. Samples per second 68853.43\n",
            "Iteration 3000000 in 43.00 seconds, loss is 0.3767. Samples per second 69773.40\n",
            "Iteration 4000000 in 56.80 seconds, loss is 0.3739. Samples per second 70423.57\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.85 min. Users per second: 320\n",
            "Epoch 9: MAP@10 = 0.0411\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 10\n",
            "Iteration 0 in 0.92 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.89 seconds, loss is 0.3553. Samples per second 67143.33\n",
            "Iteration 2000000 in 28.77 seconds, loss is 0.3530. Samples per second 69528.76\n",
            "Iteration 3000000 in 42.72 seconds, loss is 0.3502. Samples per second 70222.06\n",
            "Iteration 4000000 in 56.53 seconds, loss is 0.3477. Samples per second 70763.58\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.86 min. Users per second: 319\n",
            "Epoch 10: MAP@10 = 0.0415\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 11\n",
            "Iteration 0 in 0.18 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.02 seconds, loss is 0.3308. Samples per second 71314.58\n",
            "Iteration 2000000 in 27.99 seconds, loss is 0.3288. Samples per second 71466.46\n",
            "Iteration 3000000 in 42.00 seconds, loss is 0.3266. Samples per second 71426.00\n",
            "Iteration 4000000 in 55.91 seconds, loss is 0.3244. Samples per second 71549.10\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.87 min. Users per second: 318\n",
            "Epoch 11: MAP@10 = 0.0417\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 12\n",
            "Iteration 0 in 0.84 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.82 seconds, loss is 0.3100. Samples per second 67469.60\n",
            "Iteration 2000000 in 28.83 seconds, loss is 0.3079. Samples per second 69361.74\n",
            "Iteration 3000000 in 42.80 seconds, loss is 0.3059. Samples per second 70100.77\n",
            "Iteration 4000000 in 56.70 seconds, loss is 0.3039. Samples per second 70548.84\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.87 min. Users per second: 317\n",
            "Epoch 12: MAP@10 = 0.0420\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 13\n",
            "Iteration 0 in 0.36 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.43 seconds, loss is 0.2903. Samples per second 69308.99\n",
            "Iteration 2000000 in 28.34 seconds, loss is 0.2886. Samples per second 70561.68\n",
            "Iteration 3000000 in 42.96 seconds, loss is 0.2868. Samples per second 69829.91\n",
            "Iteration 4000000 in 56.96 seconds, loss is 0.2851. Samples per second 70222.12\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.87 min. Users per second: 317\n",
            "Epoch 13: MAP@10 = 0.0422\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 14\n",
            "Iteration 0 in 1.04 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 15.07 seconds, loss is 0.2731. Samples per second 66358.63\n",
            "Iteration 2000000 in 28.93 seconds, loss is 0.2715. Samples per second 69136.21\n",
            "Iteration 3000000 in 42.83 seconds, loss is 0.2699. Samples per second 70048.30\n",
            "Iteration 4000000 in 56.78 seconds, loss is 0.2683. Samples per second 70446.50\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.88 min. Users per second: 315\n",
            "Epoch 14: MAP@10 = 0.0422\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 15\n",
            "Iteration 0 in 0.38 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.12 seconds, loss is 0.2579. Samples per second 55195.64\n",
            "Iteration 2000000 in 36.60 seconds, loss is 0.2564. Samples per second 54651.94\n",
            "Iteration 3000000 in 54.31 seconds, loss is 0.2548. Samples per second 55239.38\n",
            "Iteration 4000000 in 72.27 seconds, loss is 0.2533. Samples per second 55346.66\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.25 min. Users per second: 264\n",
            "Epoch 15: MAP@10 = 0.0422\n",
            "No improvement in MAP@10. Patience counter: 1/5\n",
            "Starting epoch 16\n",
            "Iteration 0 in 0.69 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.48 seconds, loss is 0.2433. Samples per second 54099.08\n",
            "Iteration 2000000 in 36.66 seconds, loss is 0.2419. Samples per second 54558.39\n",
            "Iteration 3000000 in 54.21 seconds, loss is 0.2405. Samples per second 55343.63\n",
            "warm restart\n",
            "Early stopping at iteration 3950000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.26 min. Users per second: 262\n",
            "Epoch 16: MAP@10 = 0.0423\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 17\n",
            "Iteration 0 in 1.06 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.92 seconds, loss is 0.2331. Samples per second 52854.40\n",
            "Iteration 2000000 in 36.67 seconds, loss is 0.2319. Samples per second 54535.53\n",
            "Iteration 3000000 in 54.28 seconds, loss is 0.2306. Samples per second 55267.41\n",
            "Iteration 4000000 in 72.03 seconds, loss is 0.2293. Samples per second 55536.15\n",
            "warm restart\n",
            "Early stopping at iteration 4810000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.24 min. Users per second: 265\n",
            "Epoch 17: MAP@10 = 0.0423\n",
            "No improvement in MAP@10. Patience counter: 1/5\n",
            "Starting epoch 18\n",
            "Iteration 0 in 0.57 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.41 seconds, loss is 0.2211. Samples per second 54323.55\n",
            "Iteration 2000000 in 36.77 seconds, loss is 0.2200. Samples per second 54395.94\n",
            "Iteration 3000000 in 54.21 seconds, loss is 0.2189. Samples per second 55335.49\n",
            "Iteration 4000000 in 71.93 seconds, loss is 0.2177. Samples per second 55608.71\n",
            "warm restart\n",
            "Early stopping at iteration 4370000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.25 min. Users per second: 263\n",
            "Epoch 18: MAP@10 = 0.0423\n",
            "Improvement detected. Saving the current similarity matrix.\n",
            "Starting epoch 19\n",
            "Iteration 0 in 1.17 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.99 seconds, loss is 0.2112. Samples per second 52660.62\n",
            "Iteration 2000000 in 36.40 seconds, loss is 0.2103. Samples per second 54948.86\n",
            "Iteration 3000000 in 54.37 seconds, loss is 0.2092. Samples per second 55179.76\n",
            "warm restart\n",
            "Early stopping at iteration 3755000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.26 min. Users per second: 263\n",
            "Epoch 19: MAP@10 = 0.0423\n",
            "No improvement in MAP@10. Patience counter: 1/5\n",
            "Starting epoch 20\n",
            "Iteration 0 in 0.74 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.40 seconds, loss is 0.2036. Samples per second 54339.47\n",
            "Iteration 2000000 in 36.22 seconds, loss is 0.2025. Samples per second 55213.22\n",
            "warm restart\n",
            "Iteration 3000000 in 53.78 seconds, loss is 0.2016. Samples per second 55779.61\n",
            "Early stopping at iteration 3095000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.26 min. Users per second: 262\n",
            "Epoch 20: MAP@10 = 0.0423\n",
            "No improvement in MAP@10. Patience counter: 2/5\n",
            "Starting epoch 21\n",
            "Iteration 0 in 0.72 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.32 seconds, loss is 0.1975. Samples per second 54574.49\n",
            "Iteration 2000000 in 35.98 seconds, loss is 0.1963. Samples per second 55586.07\n",
            "Iteration 3000000 in 53.67 seconds, loss is 0.1953. Samples per second 55901.46\n",
            "warm restart\n",
            "Early stopping at iteration 3310000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.26 min. Users per second: 263\n",
            "Epoch 21: MAP@10 = 0.0422\n",
            "No improvement in MAP@10. Patience counter: 3/5\n",
            "Starting epoch 22\n",
            "Iteration 0 in 1.15 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 18.81 seconds, loss is 0.1915. Samples per second 53150.62\n",
            "Iteration 2000000 in 36.15 seconds, loss is 0.1904. Samples per second 55320.20\n",
            "warm restart\n",
            "Iteration 3000000 in 53.66 seconds, loss is 0.1894. Samples per second 55910.53\n",
            "Early stopping at iteration 3190000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 2.10 min. Users per second: 282\n",
            "Epoch 22: MAP@10 = 0.0423\n",
            "No improvement in MAP@10. Patience counter: 4/5\n",
            "Starting epoch 23\n",
            "Iteration 0 in 0.24 seconds, loss is 0.0000. Samples per second 0.00\n",
            "Iteration 1000000 in 14.72 seconds, loss is 0.1851. Samples per second 67932.84\n",
            "Iteration 2000000 in 29.62 seconds, loss is 0.1845. Samples per second 67512.07\n",
            "warm restart\n",
            "Iteration 3000000 in 43.80 seconds, loss is 0.1836. Samples per second 68493.97\n",
            "Early stopping at iteration 3065000. Loss has not improved significantly for 100000 iterations, or has stayed above 1 too much\n",
            "EvaluatorHoldout: Processed 35591 (100.0%) in 1.89 min. Users per second: 314\n",
            "Epoch 23: MAP@10 = 0.0422\n",
            "No improvement in MAP@10. Patience counter: 5/5\n",
            "Early stopping: No improvement in MAP@10 for consecutive epochs.\n",
            "Training complete. Best MAP@10: 0.0423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "# Initialize the recommender with the best similarity matrix\n",
        "recommender = MyItemItemRecommender(URM_train, best_item_item_S) #usare betst_item_item_S\n",
        "recommender.fit()\n",
        "\n",
        "# Create an evaluator for the test set\n",
        "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])  # Evaluate MAP@10\n",
        "\n",
        "# Evaluate the recommender on the test set\n",
        "results_test, _ = evaluator_test.evaluateRecommender(recommender)\n",
        "\n",
        "# Print results\n",
        "print(\"Test Results:\")\n",
        "for cutoff in results_test.index:\n",
        "    print(f\"Cutoff {cutoff}:\")\n",
        "    print(f\"MAP: {results_test.loc[cutoff, 'MAP']:.4f}\")\n",
        "    print(f\"Precision: {results_test.loc[cutoff, 'PRECISION']:.4f}\")\n",
        "    print(f\"Recall: {results_test.loc[cutoff, 'RECALL']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iQJdd6mHlQT",
        "outputId": "9b92e743-d530-4d67-d9bc-7b6df3293c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EvaluatorHoldout: Ignoring 149 ( 0.4%) Users that have less than 1 test interactions\n",
            "EvaluatorHoldout: Processed 35587 (100.0%) in 1.89 min. Users per second: 314\n",
            "Test Results:\n",
            "Cutoff 10:\n",
            "MAP: 0.2944\n",
            "Precision: 0.3697\n",
            "Recall: 0.4656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from Evaluation.Evaluator import EvaluatorHoldout\n",
        "\n",
        "# Step 1: Compute user interaction counts using URM_all\n",
        "user_interaction_counts = np.ediff1d(URM_all.tocsr().indptr)\n",
        "\n",
        "# Step 2: Define user activity thresholds\n",
        "low_activity_threshold = np.percentile(user_interaction_counts, 33)\n",
        "high_activity_threshold = np.percentile(user_interaction_counts, 66)\n",
        "\n",
        "# Step 3: Create boolean masks and get user IDs\n",
        "user_ids = np.arange(URM_all.shape[0])\n",
        "\n",
        "low_activity_users = user_interaction_counts <= low_activity_threshold\n",
        "medium_activity_users = (user_interaction_counts > low_activity_threshold) & (user_interaction_counts <= high_activity_threshold)\n",
        "high_activity_users = user_interaction_counts > high_activity_threshold\n",
        "\n",
        "low_activity_user_ids = user_ids[low_activity_users]\n",
        "medium_activity_user_ids = user_ids[medium_activity_users]\n",
        "high_activity_user_ids = user_ids[high_activity_users]\n",
        "\n",
        "# Initialize the recommender with the trained similarity matrix\n",
        "recommender = MyItemItemRecommender(URM_train, best_item_item_S)\n",
        "recommender.fit()\n",
        "\n",
        "# Function to evaluate for a user group\n",
        "def evaluate_for_user_group(user_group_ids, group_name):\n",
        "    # Ensure the user IDs are valid (i.e., they exist in URM_test)\n",
        "    all_user_ids = np.arange(URM_test.shape[0])\n",
        "    valid_user_ids = np.intersect1d(user_group_ids, all_user_ids)\n",
        "\n",
        "    # Check if there are any interactions in the test set for the user group\n",
        "    num_interactions = URM_test[valid_user_ids].nnz\n",
        "    if num_interactions == 0:\n",
        "        print(f\"No test interactions for {group_name} activity users.\")\n",
        "        return\n",
        "\n",
        "    # Create a list of users to ignore: all users except the ones in the group\n",
        "    ignore_users = np.setdiff1d(all_user_ids, valid_user_ids)\n",
        "\n",
        "    # Create an evaluator that ignores all other users\n",
        "    evaluator_group = EvaluatorHoldout(URM_test, cutoff_list=[10], ignore_users=ignore_users.tolist())\n",
        "\n",
        "    # Evaluate the recommender on the user group\n",
        "    results_group, _ = evaluator_group.evaluateRecommender(recommender)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Results for {group_name} Activity Users:\")\n",
        "    for cutoff in results_group.index:\n",
        "        print(f\"Cutoff {cutoff}:\")\n",
        "        print(f\"MAP: {results_group.loc[cutoff, 'MAP']:.4f}\")\n",
        "        print(f\"Precision: {results_group.loc[cutoff, 'PRECISION']:.4f}\")\n",
        "        print(f\"Recall: {results_group.loc[cutoff, 'RECALL']:.4f}\")\n",
        "        print(f\"NDCG: {results_group.loc[cutoff, 'NDCG']:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluate for each user group\n",
        "evaluate_for_user_group(low_activity_user_ids, \"Low\")\n",
        "evaluate_for_user_group(medium_activity_user_ids, \"Medium\")\n",
        "evaluate_for_user_group(high_activity_user_ids, \"High\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KNj7OfHhcEH",
        "outputId": "ad2bb682-8686-41f7-e67c-4c6e8752421b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EvaluatorHoldout: Ignoring 149 ( 0.4%) Users that have less than 1 test interactions\n",
            "EvaluatorHoldout: Ignoring 23483 Users\n",
            "EvaluatorHoldout: Processed 12118 (100.0%) in 33.45 sec. Users per second: 362\n",
            "Results for Low Activity Users:\n",
            "Cutoff 10:\n",
            "MAP: 0.1216\n",
            "Precision: 0.1907\n",
            "Recall: 0.4720\n",
            "NDCG: 0.4128\n",
            "\n",
            "\n",
            "EvaluatorHoldout: Ignoring 149 ( 0.4%) Users that have less than 1 test interactions\n",
            "EvaluatorHoldout: Ignoring 24267 Users\n",
            "EvaluatorHoldout: Processed 11456 (100.0%) in 36.20 sec. Users per second: 316\n",
            "Results for Medium Activity Users:\n",
            "Cutoff 10:\n",
            "MAP: 0.2251\n",
            "Precision: 0.3070\n",
            "Recall: 0.5072\n",
            "NDCG: 0.5053\n",
            "\n",
            "\n",
            "EvaluatorHoldout: Ignoring 149 ( 0.4%) Users that have less than 1 test interactions\n",
            "EvaluatorHoldout: Ignoring 23722 Users\n",
            "EvaluatorHoldout: Processed 12013 (100.0%) in 1.14 min. Users per second: 175\n",
            "Results for High Activity Users:\n",
            "Cutoff 10:\n",
            "MAP: 0.5348\n",
            "Precision: 0.6099\n",
            "Recall: 0.4195\n",
            "NDCG: 0.6723\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}