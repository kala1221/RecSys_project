Every result is to be considered as MAP@10

--SLIM_EN (no contribution from ICM):
(
	delta = learning_rate * (
        prediction_error * profile_rating
        - (1 - l1_ratio_val) * regularization_2_val * item_item_S[profile_item_id, item_id]
        - l1_ratio_val * sign(item_item_S[profile_item_id, item_id])
        )
        item_item_S[profile_item_id, item_id] += delta
        item_item_S[item_id, profile_item_id] += delta
)
initial_learning_rate= 0.00010012085679135815
regularization = 2.668605667480152e-06
l1_ratio = 0.24613633537942015
decay_rate = 0.9999991522253808
Got as results :
val set => 0.0424


--SLIM_EN with cosine similarity from ICM included into loss:
(
	delta = learning_rate * (
        prediction_error * profile_rating
        - (1 - l1_ratio) * regularization_2 * item_item_S[profile_item_id, item_id]
        - l1_ratio * sign(item_item_S[profile_item_id, item_id])
        - alpha * (item_item_S[profile_item_id, item_id] - S_icm[profile_item_id, item_id]) #new for icm
        item_item_S[profile_item_id, item_id] += delta
        item_item_S[item_id, profile_item_id] += delta
           
)	

'alpha': 0.01808726112039536,
'learning_rate': 0.00010202373981397776 
'regularization': 1.8374449620346475e-05
'decay_rate': 0.9999996279382791
'l1_ratio': 0.034021854645934185
Got as results:
val set => 0.0423


-- ItemKNN_CFCBF_Hybrid
'ICM_weight': 0.30000000000000004 
'topK': 10
'shrink': 0
'similarity': 'cosine'
'normalize': True
'feature_weighting': 'BM25'
'ICM_bias': False
Got as results:
test set => 0.0512

-- SLIM_BPR
'epochs': 450
'lambda_i': 0.0009444425952309716
'lambda_j': 2.0819719412921137e-05
'learning_rate': 0.08553888450970083
'topK': 50
'sgd_mode': 'adagrad'
'gamma': 0.96
'beta_1': 0.7
'beta_2': 0.7
val set => 0.0433

-- SLIM WARP
'learning_rate': 0.00011992400986377744, 
'regularization': 7.224138835853177e-06, 
'decay_rate': 0.9999993638196324
'gamma': 4.434871428314343}. 
Got as results:
val set => 0.0461
on the hidden test set =>0.3473

--mVAE
{'epochs': 79,
 'batch_size': 418,
 'learning_rate': 0.0008864400110679824,
 'l2_reg': 7.142052814300075e-05,
 'dropout': 0.10675630269315449,
 'anneal_cap': 0.3574690571170327,
 'total_anneal_steps': 118991}
val set =>  0.016131781278140567
