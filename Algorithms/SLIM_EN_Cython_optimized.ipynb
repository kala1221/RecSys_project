{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrZXMmb5hp57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ZBjXoIvHsk"
      },
      "outputs": [],
      "source": [
        "#non questo\n",
        "%%cython\n",
        "import numpy as np\n",
        "cimport numpy as np  # Import C numpy for typed memoryviews\n",
        "import time\n",
        "np.random.seed(42)\n",
        "cdef double sign(double x):\n",
        "          if x > 0:\n",
        "              return 1.0\n",
        "          elif x < 0:\n",
        "              return -1.0\n",
        "          else:\n",
        "              return 0.0\n",
        "def do_some_training_EN(URM_train,\n",
        "                     double initial_learning_rate,\n",
        "                     double regularization_2,\n",
        "                     double l1_ratio,  # ElasticNet's L1 ratio\n",
        "                     double decay_rate,\n",
        "                     int num_iterations,\n",
        "                     np.float64_t[:, :] existing_item_item_S):\n",
        "\n",
        "    # Ensure URM_train is in CSR format for efficient row slicing\n",
        "    URM_train_csr = URM_train.tocsr()\n",
        "    URM_train_coo = URM_train.tocoo()\n",
        "    n_items = URM_train.shape[1]\n",
        "\n",
        "    cdef np.float64_t[:, :] item_item_S\n",
        "    # Use the provided item-item similarity matrix if available; otherwise, initialize with zeros\n",
        "    if existing_item_item_S is not None:\n",
        "        item_item_S = existing_item_item_S\n",
        "    else:\n",
        "        item_item_S = np.zeros((n_items, n_items), dtype=np.float64)\n",
        "\n",
        "    # Initial learning rate and regularization\n",
        "    cdef np.float64_t learning_rate = initial_learning_rate\n",
        "    cdef np.float64_t regularization_2_val = regularization_2\n",
        "    cdef np.float64_t l1_ratio_val = l1_ratio\n",
        "    cdef np.float64_t loss = 0.0\n",
        "    cdef long start_time = time.time()\n",
        "    cdef np.float64_t true_rating, predicted_rating, prediction_error, profile_rating\n",
        "    cdef np.int32_t[:] items_in_user_profile\n",
        "    cdef np.float64_t[:] ratings_in_user_profile\n",
        "    cdef int index, sample_num, user_id, item_id, profile_item_id, sample_index\n",
        "\n",
        "    cdef np.float64_t decay_rate_val = decay_rate\n",
        "    # Initialize a list to store loss values\n",
        "    cdef list loss_history = []\n",
        "\n",
        "    # Early stopping variables\n",
        "    cdef int patience_counter = 0\n",
        "    cdef double last_loss = np.inf\n",
        "    cdef int patience = 20\n",
        "    cdef double min_delta = 1e-5\n",
        "    cdef int warm_restart = 0\n",
        "\n",
        "    cdef double delta=0\n",
        "\n",
        "    cdef int[:] random_indices = np.random.randint(0, URM_train_coo.nnz, size=num_iterations).astype(np.int32)\n",
        "    # Memoryviews for efficient access\n",
        "    cdef int[:] indices = URM_train_csr.indices\n",
        "    cdef int[:] indptr = URM_train_csr.indptr\n",
        "    cdef double[:] data = URM_train_csr.data\n",
        "    cdef int[:] coo_row = URM_train_coo.row\n",
        "    cdef int[:] coo_col = URM_train_coo.col\n",
        "    cdef double[:] coo_data = URM_train_coo.data\n",
        "    # Use `indices`, `indptr`, and `data` within the loop\n",
        "\n",
        "    for sample_num in range(num_iterations):\n",
        "        sample_index = random_indices[sample_num]\n",
        "        user_id = coo_row[sample_index]\n",
        "        item_id = coo_col[sample_index]\n",
        "        true_rating = coo_data[sample_index]\n",
        "\n",
        "        # Compute prediction\n",
        "        items_in_user_profile = indices[indptr[user_id]:indptr[user_id+1]]\n",
        "        ratings_in_user_profile = data[indptr[user_id]:indptr[user_id+1]]\n",
        "        predicted_rating = 0.0\n",
        "\n",
        "        for index in range(len(items_in_user_profile)):\n",
        "            profile_item_id = items_in_user_profile[index]\n",
        "            profile_rating = ratings_in_user_profile[index]\n",
        "            predicted_rating += profile_rating * item_item_S[profile_item_id, item_id]\n",
        "\n",
        "        # Compute prediction error\n",
        "        prediction_error = true_rating - predicted_rating\n",
        "        loss += prediction_error ** 2\n",
        "\n",
        "        # Update model (ElasticNet: L2 and L1 penalties)\n",
        "        for index in range(len(items_in_user_profile)):\n",
        "            profile_item_id = items_in_user_profile[index]\n",
        "            profile_rating = ratings_in_user_profile[index]\n",
        "            delta = learning_rate * (\n",
        "              prediction_error * profile_rating\n",
        "              - (1 - l1_ratio_val) * regularization_2_val * item_item_S[profile_item_id, item_id]\n",
        "              - l1_ratio_val * sign(item_item_S[profile_item_id, item_id])\n",
        "            )\n",
        "            item_item_S[profile_item_id, item_id] += delta\n",
        "            item_item_S[item_id, profile_item_id] += delta\n",
        "\n",
        "        # Print some stats\n",
        "        if sample_num % 5000 == 0:\n",
        "            learning_rate *= decay_rate_val\n",
        "            current_loss = loss / sample_num\n",
        "            loss_history.append(current_loss)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            samples_per_second = sample_num / elapsed_time\n",
        "            print(\n",
        "                \"Iteration {} in {:.2f} seconds, loss is {:.4f}. Samples per second {:.2f}\".format(\n",
        "                    sample_num, elapsed_time, current_loss, samples_per_second\n",
        "                )\n",
        "            )\n",
        "            # Early stopping check\n",
        "            if abs(last_loss - current_loss) < min_delta or current_loss > 1:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    if warm_restart == 0:\n",
        "                        learning_rate = initial_learning_rate\n",
        "                        warm_restart += 1\n",
        "                        patience_counter = 0\n",
        "                        print(\"warm restart\")\n",
        "                    else:\n",
        "                        print(\"Early stopping at iteration {}. Loss has not improved significantly for {} iterations, or has stayed above 1 too much\".format(sample_num, patience * 5000))\n",
        "                        break\n",
        "            else:\n",
        "                patience_counter = 0  # Reset patience counter if loss improves\n",
        "            last_loss = current_loss\n",
        "            # Imposta la diagonale a zero se necessario, ogni 5000\n",
        "            for i in range(n_items):\n",
        "                item_item_S[i, i] = 0.0\n",
        "    item_item_S = (item_item_S + item_item_S.T) / 2\n",
        "    return loss, samples_per_second, item_item_S, loss_history\n"
      ]
    }
  ]
}